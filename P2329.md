# D2329R0: Move, Copy, and Locality at Scale

Pablo Halpern, <!-- $TimeStamp$ --> 2021-12-06 21:02 EST <!-- $TimeStamp$ -->

## Abstract

This paper (it is not a proposal) explores the performance benefits and
penalties of preferring move assignment over. copy assignment on containers
that allocate memory. A relatively simple experiment is described whereby
elements within a subsystem are allocated consecutively, in contiguous
memory. The elements are then shuffled across subsystem using either move
assignment or copy assignment, then accessed (read-only) repeatedly within each
subsystem. Preliminary results show that, although the shuffle step is
generally faster when using move assignment, especially for large elements,
cache effects can more than cancel out the benefit under certain circumstances
when the overall system does not fit in L3 cache. Moreover, when the system
size approaches or exceeds the size of physical memory, data within a subsystem
is spread out over many more pages, sometimes resulting in a dramatic slowdown
for accessing the data when it was shuffled using move assignment vs. copy
assignment. Users should thus be aware that preferring moves to copies does not
always result in a performance increase and might, for large programs, do the
opposite.

## Introduction

We designed an experiment intended to simulate a "system" composed of multiple
"subsystems."  Each subsystem creates multiple data "elements", each of which
allocate memory. The simulation assumes that these elements are mostly accessed
(read-only) within the subsystem, but are occasionally transferred among
subsystems. This transfer can be accomplished using either move assignment or
copy assignment, where move assignment is optimized to transfer ownership of
the allocated memory whereas copy assignment copies the contents of the
allocated bytes.

The question we were trying to answer is if, and under what conditions, does
the use of move assignment yield a performance benefit over move assignment and
vice versa. To answer this question, the simulation is parameterized on the
number of subsystems in the system, the number of elements in each subsystem,
the size (in bytes) of each element, the number of times the elements are
"churned" (shuffled between subsystems), the number of times every element in
each subsystem is accessed, and the number of times the entire churn/access
cycle is repeated. The entire simulation is run once using copy assignment in
the churn step and then again using move assignment.  Each simulated run is
timed and the times are reported as well as the quotient (expressed as a
percent) obtained by dividing the move time by the copy time. By varying these
parameters we can find patterns for when move assignment was beneficial
(computed percentage significantly less than 100), when it was detrimental
(computed percentage significantly greater than 100), and when the difference
was within the noise (computed percentage close to 100).

Because the parameter space is large (7 parameters), and because some runtimes
were long (in access of an hour), it was not possible to cover the entire
parameter space.  Therefore the simulation was run within a script that varied
two or three parameters at a time.  The total system size, in bytes, was
computed by multiplying the number of subsystems, the number of elements per
subsystem, and the element size. In many cases the script held the total system
size constant by varying the number of subsystems and the number of elements
per subsystem in inverse proportions.

## Observations

Although this is not a comprehensive analysis, the data yielded the following
observations:

* With 128-byte elements and a 32MiB total system size, move assignment yielded
  a up to a 2x speedup (50% run time) with a large number of small subsystems
  but a 2x slowdown (189% run time) with a small number of large subsystems.

* With 64-byte elements and 4GiB or 8GiB total system size, move assignment was
  typically worse than copy assignment (up to 7x worse), but even in this case,
  we saw huge speed-ups with large numbers of small subsystems.

* The results were somewhat noisy, i.e., in many cases, consecutive runs with
  only small parameter changes resulted in large swings.  One theory is that
  the alignment of elements on cache lines was different between such runs.
  Nevertheless, certain patterns did emerge, such as the large slowdowns caused
  by page thrashing.

## Technical information

Source for this micro-benchmark can be found at
[https://github.com/phalpern/WG21-halpern/tree/P2329/P2329-move_at_scale](https://github.com/phalpern/WG21-halpern/tree/P2329/P2329-move_at_scale).

Results, as a set of `.csv` files, can be found in the
[R0-results](https://github.com/phalpern/WG21-halpern/tree/P2329/P2329_copy_move_benchmark/R0-results)
subdirectory.

The results were generated on a MacBook Pro (15 inch, 2018) with the following
specifications. (source:
[GeekBench Browser](https://browser.geekbench.com/v4/compute/2624464):

* Model: MacBook Pro 2018 (Model ID: MacBookPro15,1)
* CPU: 6-core Intel Core i7, 2.2 GHz
* L1 Data Cache: 32KiB per core
* L1 Instruction Cache: 32KiB per core
* L2 Cache 256KiB per core
* L3 Cache 9MiB shared
* RAM: 16GiB


## Limitations of this experiment

We recognize that this experiment is not the final word on this
subject. Specifically, we are considering making the following improvements:

* Run on more different types of hardware.
* Experiment with element sizes that are not necessarily uniform.
* When computing the total system size, take into the account that the system,
  subsystems, and elements are all `vector` types and have their own footprints.
* Try a memory resource that allocates items in the smallest possible number of
  cache lines.
* Run experiments where subsystem accesses run in concurrent threads.
* Although the parameter space is large and some runtimes are very long, we can
  and should dedicate more run time to cover a larger portion of the (carefully
  curated) parameter space.

It is unclear whether, and to what degree, our simulation reflects real world
programs. Measurements on real, memory-bound programs would be ideal.

## Conclusions

* Using move assignment instead of copy results in a performance improvement,
  sometimes dramatically so, in great many circumstances with non-trivial
  element sizes.
* However, when to the total system gets very large, such that individual
  subsystems exceed the size of L3 cache, copy can provide better spacial
  locality when the number of accesses to the data exceeds the number of moves
  by a factor of 4 or more.
* The deleterious effect of non-locality on performance caused by using move
  assignment is even more dramatic if the total system exceeds the size of
  physical memory.
* When the number of accesses greatly exceeds the number of moves, the benefits
  of move assignment are minimized, but the potential downside on performance is
  still potentially large.
* These effects are not always predictable. As always, the best approach is to
  measure.
* One should not assume that move assignment is preferable to copy assignment
  in large-scale programs.
